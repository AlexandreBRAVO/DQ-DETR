fatal: ambiguous argument 'HEAD': unknown revision or path not in the working tree.
Use '--' to separate paths from revisions, like this:
'git <command> [<revision>...] -- [<file>...]'
Not using distributed mode
[08/14 21:31:39.264]: git:
  sha: N/A, status: clean, branch: N/A

[08/14 21:31:39.264]: Command: main_aitod.py --output_dir logs/DQ/R50-MS4-%j -c config/DQ_5scale.py --coco_path /mnt/data0/Garmin/datasets --eval --resume /mnt/data0/Garmin/dndetr/DQ-DETR/best305_new.pth --options dn_scalar=100 embed_init_tgt=False dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False dn_box_noise_scale=1.0
[08/14 21:31:39.264]: Full config saved to logs/DQ/R50-MS4-%j/config_args_all.json
[08/14 21:31:39.264]: world size: 1
[08/14 21:31:39.264]: rank: 0
[08/14 21:31:39.264]: local_rank: 0
[08/14 21:31:39.265]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=1, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/mnt/data0/Garmin/datasets', config_file='config/DQ_5scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='aitod_v2', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_bbox_coef=1.0, dn_box_noise_scale=1.0, dn_label_coef=1.0, dn_label_noise_ratio=0.5, dn_labelbook_size=91, dn_number=100, dn_scalar=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=12, eval=True, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[11, 23], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='dqdetr', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=9, num_feature_levels=5, num_patterns=0, num_queries=900, num_select=300, num_workers=10, onecyclelr=False, options={'dn_scalar': 100, 'embed_init_tgt': False, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, output_dir='logs/DQ/R50-MS4-%j', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='/mnt/data0/Garmin/dndetr/DQ-DETR/best305_new.pth', return_interm_indices=[0, 1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

[08/14 21:31:41.886]: number of params:58702866
[08/14 21:31:41.888]: params:
{
  "transformer.level_embed": 1280,
  "transformer.CCM.conv1.weight": 131072,
  "transformer.CCM.conv1.bias": 512,
  "transformer.CCM.ccm.0.weight": 2359296,
  "transformer.CCM.ccm.0.bias": 512,
  "transformer.CCM.ccm.2.weight": 2359296,
  "transformer.CCM.ccm.2.bias": 512,
  "transformer.CCM.ccm.4.weight": 2359296,
  "transformer.CCM.ccm.4.bias": 512,
  "transformer.CCM.ccm.6.weight": 1179648,
  "transformer.CCM.ccm.6.bias": 256,
  "transformer.CCM.ccm.8.weight": 589824,
  "transformer.CCM.ccm.8.bias": 256,
  "transformer.CCM.ccm.10.weight": 589824,
  "transformer.CCM.ccm.10.bias": 256,
  "transformer.CCM.linear.weight": 1024,
  "transformer.CCM.linear.bias": 4,
  "transformer.CGFE.ChannelGate.mlp.1.weight": 4096,
  "transformer.CGFE.ChannelGate.mlp.1.bias": 16,
  "transformer.CGFE.ChannelGate.mlp.3.weight": 4096,
  "transformer.CGFE.ChannelGate.mlp.3.bias": 256,
  "transformer.CGFE.SpatialGate.spatial.conv.weight": 98,
  "transformer.CGFE.SpatialGate.spatial.bn.weight": 1,
  "transformer.CGFE.SpatialGate.spatial.bn.bias": 1,
  "transformer.multiscale.conv1.conv.weight": 589824,
  "transformer.multiscale.conv1.gn.weight": 256,
  "transformer.multiscale.conv1.gn.bias": 256,
  "transformer.multiscale.conv2.conv.weight": 589824,
  "transformer.multiscale.conv2.gn.weight": 256,
  "transformer.multiscale.conv2.gn.bias": 256,
  "transformer.multiscale.conv3.conv.weight": 589824,
  "transformer.multiscale.conv3.gn.weight": 256,
  "transformer.multiscale.conv3.gn.bias": 256,
  "transformer.multiscale.conv4.conv.weight": 589824,
  "transformer.multiscale.conv4.gn.weight": 256,
  "transformer.multiscale.conv4.gn.bias": 256,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 81920,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 320,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 40960,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 160,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 81920,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 320,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 40960,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 160,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 81920,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 320,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 40960,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 160,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 81920,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 320,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 40960,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 160,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 81920,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 320,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 40960,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 160,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 81920,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 320,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 40960,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 160,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 81920,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 320,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 40960,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 160,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 81920,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 320,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 40960,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 160,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 81920,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 320,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 40960,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 160,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 81920,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 320,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 40960,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 160,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 81920,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 320,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 40960,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 160,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 81920,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 320,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 40960,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 160,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.class_embed.0.weight": 2304,
  "transformer.decoder.class_embed.0.bias": 9,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 2304,
  "transformer.enc_out_class_embed.bias": 9,
  "label_enc.weight": 23552,
  "input_proj.0.0.weight": 65536,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 131072,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 262144,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 524288,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "input_proj.4.0.weight": 4718592,
  "input_proj.4.0.bias": 256,
  "input_proj.4.1.weight": 256,
  "input_proj.4.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
data_aug_params: {
  "scales": [
    480,
    512,
    544,
    576,
    608,
    640,
    672,
    704,
    736,
    768,
    800
  ],
  "max_size": 1333,
  "scales2_resize": [
    400,
    500,
    600
  ],
  "scales2_crop": [
    384,
    600
  ]
}
loading annotations into memory...
Done (t=3.17s)
creating index...
index created!
data_aug_params: {
  "scales": [
    480,
    512,
    544,
    576,
    608,
    640,
    672,
    704,
    736,
    768,
    800
  ],
  "max_size": 1333,
  "scales2_resize": [
    400,
    500,
    600
  ],
  "scales2_crop": [
    384,
    600
  ]
}
loading annotations into memory...
Done (t=3.48s)
creating index...
index created!
/mnt/data0/Garmin/anaconda3/envs/dndetr/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/data0/Garmin/anaconda3/envs/dndetr/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Test:  [    0/14018]  eta: 10:29:35  class_error: 0.00  loss: 4.6375 (4.6375)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.0763 (0.0763)  loss_bbox: 0.0178 (0.0178)  loss_giou: 0.5595 (0.5595)  loss_ce_0: 0.0949 (0.0949)  loss_bbox_0: 0.0162 (0.0162)  loss_giou_0: 0.5475 (0.5475)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.0895 (0.0895)  loss_bbox_1: 0.0173 (0.0173)  loss_giou_1: 0.5719 (0.5719)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.0792 (0.0792)  loss_bbox_2: 0.0175 (0.0175)  loss_giou_2: 0.5624 (0.5624)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.0814 (0.0814)  loss_bbox_3: 0.0178 (0.0178)  loss_giou_3: 0.5647 (0.5647)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.0722 (0.0722)  loss_bbox_4: 0.0179 (0.0179)  loss_giou_4: 0.5615 (0.5615)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.0808 (0.0808)  loss_bbox_interm: 0.0179 (0.0179)  loss_giou_interm: 0.5732 (0.5732)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.0763 (0.0763)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0036 (0.0036)  loss_giou_unscaled: 0.2798 (0.2798)  loss_xy_unscaled: 0.0011 (0.0011)  loss_hw_unscaled: 0.0025 (0.0025)  cardinality_error_unscaled: 293.0000 (293.0000)  loss_ce_0_unscaled: 0.0949 (0.0949)  loss_bbox_0_unscaled: 0.0032 (0.0032)  loss_giou_0_unscaled: 0.2737 (0.2737)  loss_xy_0_unscaled: 0.0011 (0.0011)  loss_hw_0_unscaled: 0.0022 (0.0022)  cardinality_error_0_unscaled: 293.0000 (293.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.0895 (0.0895)  loss_bbox_1_unscaled: 0.0035 (0.0035)  loss_giou_1_unscaled: 0.2860 (0.2860)  loss_xy_1_unscaled: 0.0011 (0.0011)  loss_hw_1_unscaled: 0.0023 (0.0023)  cardinality_error_1_unscaled: 293.0000 (293.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.0792 (0.0792)  loss_bbox_2_unscaled: 0.0035 (0.0035)  loss_giou_2_unscaled: 0.2812 (0.2812)  loss_xy_2_unscaled: 0.0011 (0.0011)  loss_hw_2_unscaled: 0.0024 (0.0024)  cardinality_error_2_unscaled: 293.0000 (293.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.0814 (0.0814)  loss_bbox_3_unscaled: 0.0036 (0.0036)  loss_giou_3_unscaled: 0.2823 (0.2823)  loss_xy_3_unscaled: 0.0011 (0.0011)  loss_hw_3_unscaled: 0.0024 (0.0024)  cardinality_error_3_unscaled: 293.0000 (293.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.0722 (0.0722)  loss_bbox_4_unscaled: 0.0036 (0.0036)  loss_giou_4_unscaled: 0.2807 (0.2807)  loss_xy_4_unscaled: 0.0011 (0.0011)  loss_hw_4_unscaled: 0.0025 (0.0025)  cardinality_error_4_unscaled: 293.0000 (293.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.0808 (0.0808)  loss_bbox_interm_unscaled: 0.0036 (0.0036)  loss_giou_interm_unscaled: 0.2866 (0.2866)  loss_xy_interm_unscaled: 0.0012 (0.0012)  loss_hw_interm_unscaled: 0.0024 (0.0024)  cardinality_error_interm_unscaled: 293.0000 (293.0000)  time: 2.6948  data: 0.7584  max mem: 1511
Test:  [ 5000/14018]  eta: 1:23:07  class_error: 0.00  loss: 4.7618 (5.2065)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.0042 (0.1084)  loss_bbox: 0.0250 (0.0335)  loss_giou: 0.5581 (0.5936)  loss_ce_0: 0.0382 (0.1289)  loss_bbox_0: 0.0249 (0.0335)  loss_giou_0: 0.5680 (0.5956)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.0079 (0.1124)  loss_bbox_1: 0.0253 (0.0333)  loss_giou_1: 0.5602 (0.5941)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.0036 (0.1078)  loss_bbox_2: 0.0253 (0.0335)  loss_giou_2: 0.5650 (0.5933)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.0044 (0.1077)  loss_bbox_3: 0.0252 (0.0334)  loss_giou_3: 0.5596 (0.5931)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.0041 (0.1072)  loss_bbox_4: 0.0251 (0.0335)  loss_giou_4: 0.5576 (0.5932)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.0596 (0.1307)  loss_bbox_interm: 0.0242 (0.0344)  loss_giou_interm: 0.5755 (0.6052)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.0042 (0.1084)  class_error_unscaled: 0.0000 (0.8844)  loss_bbox_unscaled: 0.0050 (0.0067)  loss_giou_unscaled: 0.2791 (0.2968)  loss_xy_unscaled: 0.0014 (0.0026)  loss_hw_unscaled: 0.0035 (0.0041)  cardinality_error_unscaled: 299.0000 (374.4489)  loss_ce_0_unscaled: 0.0382 (0.1289)  loss_bbox_0_unscaled: 0.0050 (0.0067)  loss_giou_0_unscaled: 0.2840 (0.2978)  loss_xy_0_unscaled: 0.0016 (0.0025)  loss_hw_0_unscaled: 0.0035 (0.0042)  cardinality_error_0_unscaled: 299.0000 (374.4481)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.0079 (0.1124)  loss_bbox_1_unscaled: 0.0051 (0.0067)  loss_giou_1_unscaled: 0.2801 (0.2971)  loss_xy_1_unscaled: 0.0015 (0.0025)  loss_hw_1_unscaled: 0.0034 (0.0042)  cardinality_error_1_unscaled: 299.0000 (374.4489)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.0036 (0.1078)  loss_bbox_2_unscaled: 0.0051 (0.0067)  loss_giou_2_unscaled: 0.2825 (0.2967)  loss_xy_2_unscaled: 0.0015 (0.0025)  loss_hw_2_unscaled: 0.0035 (0.0042)  cardinality_error_2_unscaled: 299.0000 (374.4489)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.0044 (0.1077)  loss_bbox_3_unscaled: 0.0050 (0.0067)  loss_giou_3_unscaled: 0.2798 (0.2965)  loss_xy_3_unscaled: 0.0014 (0.0025)  loss_hw_3_unscaled: 0.0035 (0.0042)  cardinality_error_3_unscaled: 299.0000 (374.4489)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.0041 (0.1072)  loss_bbox_4_unscaled: 0.0050 (0.0067)  loss_giou_4_unscaled: 0.2788 (0.2966)  loss_xy_4_unscaled: 0.0014 (0.0025)  loss_hw_4_unscaled: 0.0035 (0.0042)  cardinality_error_4_unscaled: 299.0000 (374.4489)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.0596 (0.1307)  loss_bbox_interm_unscaled: 0.0048 (0.0069)  loss_giou_interm_unscaled: 0.2878 (0.3026)  loss_xy_interm_unscaled: 0.0017 (0.0026)  loss_hw_interm_unscaled: 0.0035 (0.0043)  cardinality_error_interm_unscaled: 299.0000 (374.4489)  time: 0.6160  data: 0.0045  max mem: 1512
Test:  [10000/14018]  eta: 0:36:58  class_error: 0.00  loss: 4.6623 (5.2700)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.0076 (0.1146)  loss_bbox: 0.0228 (0.0337)  loss_giou: 0.5825 (0.5967)  loss_ce_0: 0.0260 (0.1335)  loss_bbox_0: 0.0223 (0.0338)  loss_giou_0: 0.5825 (0.5982)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.0074 (0.1183)  loss_bbox_1: 0.0225 (0.0336)  loss_giou_1: 0.5779 (0.5976)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.0083 (0.1139)  loss_bbox_2: 0.0234 (0.0337)  loss_giou_2: 0.5771 (0.5967)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.0084 (0.1142)  loss_bbox_3: 0.0227 (0.0336)  loss_giou_3: 0.5778 (0.5962)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.0081 (0.1136)  loss_bbox_4: 0.0228 (0.0337)  loss_giou_4: 0.5828 (0.5963)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.0377 (0.1357)  loss_bbox_interm: 0.0256 (0.0346)  loss_giou_interm: 0.5988 (0.6081)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.0076 (0.1146)  class_error_unscaled: 0.0000 (0.8254)  loss_bbox_unscaled: 0.0046 (0.0067)  loss_giou_unscaled: 0.2913 (0.2983)  loss_xy_unscaled: 0.0016 (0.0026)  loss_hw_unscaled: 0.0028 (0.0042)  cardinality_error_unscaled: 298.0000 (375.4078)  loss_ce_0_unscaled: 0.0260 (0.1335)  loss_bbox_0_unscaled: 0.0045 (0.0068)  loss_giou_0_unscaled: 0.2913 (0.2991)  loss_xy_0_unscaled: 0.0016 (0.0026)  loss_hw_0_unscaled: 0.0027 (0.0042)  cardinality_error_0_unscaled: 298.0000 (375.4073)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.0074 (0.1183)  loss_bbox_1_unscaled: 0.0045 (0.0067)  loss_giou_1_unscaled: 0.2890 (0.2988)  loss_xy_1_unscaled: 0.0016 (0.0026)  loss_hw_1_unscaled: 0.0028 (0.0042)  cardinality_error_1_unscaled: 298.0000 (375.4078)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.0083 (0.1139)  loss_bbox_2_unscaled: 0.0047 (0.0067)  loss_giou_2_unscaled: 0.2885 (0.2983)  loss_xy_2_unscaled: 0.0016 (0.0026)  loss_hw_2_unscaled: 0.0028 (0.0042)  cardinality_error_2_unscaled: 298.0000 (375.4078)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.0084 (0.1142)  loss_bbox_3_unscaled: 0.0045 (0.0067)  loss_giou_3_unscaled: 0.2889 (0.2981)  loss_xy_3_unscaled: 0.0016 (0.0026)  loss_hw_3_unscaled: 0.0028 (0.0042)  cardinality_error_3_unscaled: 298.0000 (375.4078)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.0081 (0.1136)  loss_bbox_4_unscaled: 0.0046 (0.0067)  loss_giou_4_unscaled: 0.2914 (0.2982)  loss_xy_4_unscaled: 0.0016 (0.0026)  loss_hw_4_unscaled: 0.0028 (0.0042)  cardinality_error_4_unscaled: 298.0000 (375.4078)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.0377 (0.1357)  loss_bbox_interm_unscaled: 0.0051 (0.0069)  loss_giou_interm_unscaled: 0.2994 (0.3040)  loss_xy_interm_unscaled: 0.0017 (0.0026)  loss_hw_interm_unscaled: 0.0031 (0.0043)  cardinality_error_interm_unscaled: 298.0000 (375.4078)  time: 0.3733  data: 0.0045  max mem: 1545
Test:  [14017/14018]  eta: 0:00:00  class_error: 0.00  loss: 5.0566 (5.2929)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.0257 (0.1185)  loss_bbox: 0.0273 (0.0335)  loss_giou: 0.6090 (0.5973)  loss_ce_0: 0.0489 (0.1346)  loss_bbox_0: 0.0272 (0.0338)  loss_giou_0: 0.6118 (0.5988)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.0298 (0.1203)  loss_bbox_1: 0.0264 (0.0336)  loss_giou_1: 0.5866 (0.5982)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.0292 (0.1164)  loss_bbox_2: 0.0271 (0.0335)  loss_giou_2: 0.6001 (0.5970)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.0305 (0.1179)  loss_bbox_3: 0.0271 (0.0335)  loss_giou_3: 0.6052 (0.5966)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.0279 (0.1177)  loss_bbox_4: 0.0273 (0.0336)  loss_giou_4: 0.6086 (0.5970)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.0848 (0.1376)  loss_bbox_interm: 0.0274 (0.0346)  loss_giou_interm: 0.6113 (0.6088)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.0257 (0.1185)  class_error_unscaled: 0.0000 (0.8100)  loss_bbox_unscaled: 0.0055 (0.0067)  loss_giou_unscaled: 0.3045 (0.2987)  loss_xy_unscaled: 0.0020 (0.0026)  loss_hw_unscaled: 0.0035 (0.0041)  cardinality_error_unscaled: 299.0000 (375.1283)  loss_ce_0_unscaled: 0.0489 (0.1346)  loss_bbox_0_unscaled: 0.0054 (0.0068)  loss_giou_0_unscaled: 0.3059 (0.2994)  loss_xy_0_unscaled: 0.0019 (0.0026)  loss_hw_0_unscaled: 0.0035 (0.0042)  cardinality_error_0_unscaled: 299.0000 (375.1280)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.0298 (0.1203)  loss_bbox_1_unscaled: 0.0053 (0.0067)  loss_giou_1_unscaled: 0.2933 (0.2991)  loss_xy_1_unscaled: 0.0019 (0.0026)  loss_hw_1_unscaled: 0.0034 (0.0042)  cardinality_error_1_unscaled: 299.0000 (375.1283)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.0292 (0.1164)  loss_bbox_2_unscaled: 0.0054 (0.0067)  loss_giou_2_unscaled: 0.3000 (0.2985)  loss_xy_2_unscaled: 0.0020 (0.0026)  loss_hw_2_unscaled: 0.0036 (0.0041)  cardinality_error_2_unscaled: 299.0000 (375.1283)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.0305 (0.1179)  loss_bbox_3_unscaled: 0.0054 (0.0067)  loss_giou_3_unscaled: 0.3026 (0.2983)  loss_xy_3_unscaled: 0.0020 (0.0026)  loss_hw_3_unscaled: 0.0035 (0.0041)  cardinality_error_3_unscaled: 299.0000 (375.1283)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.0279 (0.1177)  loss_bbox_4_unscaled: 0.0055 (0.0067)  loss_giou_4_unscaled: 0.3043 (0.2985)  loss_xy_4_unscaled: 0.0020 (0.0026)  loss_hw_4_unscaled: 0.0035 (0.0041)  cardinality_error_4_unscaled: 299.0000 (375.1283)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.0848 (0.1376)  loss_bbox_interm_unscaled: 0.0055 (0.0069)  loss_giou_interm_unscaled: 0.3057 (0.3044)  loss_xy_interm_unscaled: 0.0015 (0.0026)  loss_hw_interm_unscaled: 0.0038 (0.0043)  cardinality_error_interm_unscaled: 299.0000 (375.1283)  time: 0.3405  data: 0.0043  max mem: 1545
Test: Total time: 2:12:46 (0.5683 s / it)
Averaged stats: /mnt/data0/Garmin/anaconda3/envs/dndetr/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/mnt/data0/Garmin/anaconda3/envs/dndetr/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
class_error: 0.00  loss: 5.0566 (5.2929)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.0257 (0.1185)  loss_bbox: 0.0273 (0.0335)  loss_giou: 0.6090 (0.5973)  loss_ce_0: 0.0489 (0.1346)  loss_bbox_0: 0.0272 (0.0338)  loss_giou_0: 0.6118 (0.5988)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.0298 (0.1203)  loss_bbox_1: 0.0264 (0.0336)  loss_giou_1: 0.5866 (0.5982)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.0292 (0.1164)  loss_bbox_2: 0.0271 (0.0335)  loss_giou_2: 0.6001 (0.5970)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.0305 (0.1179)  loss_bbox_3: 0.0271 (0.0335)  loss_giou_3: 0.6052 (0.5966)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.0279 (0.1177)  loss_bbox_4: 0.0273 (0.0336)  loss_giou_4: 0.6086 (0.5970)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.0848 (0.1376)  loss_bbox_interm: 0.0274 (0.0346)  loss_giou_interm: 0.6113 (0.6088)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.0257 (0.1185)  class_error_unscaled: 0.0000 (0.8100)  loss_bbox_unscaled: 0.0055 (0.0067)  loss_giou_unscaled: 0.3045 (0.2987)  loss_xy_unscaled: 0.0020 (0.0026)  loss_hw_unscaled: 0.0035 (0.0041)  cardinality_error_unscaled: 299.0000 (375.1283)  loss_ce_0_unscaled: 0.0489 (0.1346)  loss_bbox_0_unscaled: 0.0054 (0.0068)  loss_giou_0_unscaled: 0.3059 (0.2994)  loss_xy_0_unscaled: 0.0019 (0.0026)  loss_hw_0_unscaled: 0.0035 (0.0042)  cardinality_error_0_unscaled: 299.0000 (375.1280)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.0298 (0.1203)  loss_bbox_1_unscaled: 0.0053 (0.0067)  loss_giou_1_unscaled: 0.2933 (0.2991)  loss_xy_1_unscaled: 0.0019 (0.0026)  loss_hw_1_unscaled: 0.0034 (0.0042)  cardinality_error_1_unscaled: 299.0000 (375.1283)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.0292 (0.1164)  loss_bbox_2_unscaled: 0.0054 (0.0067)  loss_giou_2_unscaled: 0.3000 (0.2985)  loss_xy_2_unscaled: 0.0020 (0.0026)  loss_hw_2_unscaled: 0.0036 (0.0041)  cardinality_error_2_unscaled: 299.0000 (375.1283)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.0305 (0.1179)  loss_bbox_3_unscaled: 0.0054 (0.0067)  loss_giou_3_unscaled: 0.3026 (0.2983)  loss_xy_3_unscaled: 0.0020 (0.0026)  loss_hw_3_unscaled: 0.0035 (0.0041)  cardinality_error_3_unscaled: 299.0000 (375.1283)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.0279 (0.1177)  loss_bbox_4_unscaled: 0.0055 (0.0067)  loss_giou_4_unscaled: 0.3043 (0.2985)  loss_xy_4_unscaled: 0.0020 (0.0026)  loss_hw_4_unscaled: 0.0035 (0.0041)  cardinality_error_4_unscaled: 299.0000 (375.1283)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.0848 (0.1376)  loss_bbox_interm_unscaled: 0.0055 (0.0069)  loss_giou_interm_unscaled: 0.3057 (0.3044)  loss_xy_interm_unscaled: 0.0015 (0.0026)  loss_hw_interm_unscaled: 0.0038 (0.0043)  cardinality_error_interm_unscaled: 299.0000 (375.1283)
Accumulating evaluation results...
DONE (t=127.91s).
IoU metric: bbox
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1500 ] = 0.305
Average Precision  (AP) @[ IoU=0.25      | area=   all | maxDets=1500 ] = -1.000
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1500 ] = 0.692
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1500 ] = 0.227
Average Precision  (AP) @[ IoU=0.50:0.95 | area=verytiny | maxDets=1500 ] = 0.152
Average Precision  (AP) @[ IoU=0.50:0.95 | area=  tiny | maxDets=1500 ] = 0.309
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1500 ] = 0.368
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1500 ] = 0.455
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.082
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.422
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1500 ] = 0.463
Average Recall     (AR) @[ IoU=0.50:0.95 | area=verytiny | maxDets=1500 ] = 0.257
Average Recall     (AR) @[ IoU=0.50:0.95 | area=  tiny | maxDets=1500 ] = 0.473
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1500 ] = 0.512
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1500 ] = 0.589
Optimal LRP             @[ IoU=0.50      | area=   all | maxDets=1500 ] = 0.727
Optimal LRP Loc         @[ IoU=0.50      | area=   all | maxDets=1500 ] = 0.263
Optimal LRP FP          @[ IoU=0.50      | area=   all | maxDets=1500 ] = 0.250
Optimal LRP FN          @[ IoU=0.50      | area=   all | maxDets=1500 ] = 0.312
# Class-specific LRP-Optimal Thresholds # 
 [0.345 0.323 0.376 0.409 0.389 0.39  0.401 0.314]